


----

Not supported in Pandas.to_parquet()
    Must have string column names
    Name attributes on Frames

From: https://ursalabs.org/blog/2020-feather-v2/
Parquet format has become one of the “gold standard” binary file formats for data warehousing

Parquet docs
https://arrow.apache.org/docs/python/parquet.html


# notes on changing limit on file descriptors
https://wilsonmar.github.io/maximum-limits/
# PROTIP: On MacOS, the maximum number that can be specified is 12288.
# ulimit -n

https://numpy.org/neps/nep-0001-npy-format.html

https://blog.openbridge.com/how-to-be-a-hero-with-powerful-parquet-google-and-amazon-f2ae0f35ee04

https://towardsdatascience.com/how-fast-is-reading-parquet-file-with-arrow-vs-csv-with-pandas-2f8095722e94

Parquet is based on Dremel from 2010
https://storage.googleapis.com/pub-tools-public-publication-data/pdf/36632.pdf

Record shredding: is it important?
https://www.joekearney.co.uk/posts/understanding-record-shredding



----
Title:

Using the NPY Format for Faster-Than Parquet, Memory-Mappable, Complete DataFrame Serialization

Employing NumPy's NPY & NPZ File Formats for Faster-than Parquet, Memory-Mappable, Complete DataFrame Serialization

Employing NumPy's NPY Format for Faster-Than-Parquet DataFrame Serialization

----

Abstract:
    You can use Markdown here. Please do not include any personally identifiable information. The initial round of reviews are anonymous, and this field will be visible to reviewers. Please write at most 300 words.


Over 14 years ago the first NumPy Enhancement Proposal (NEP) defined the NPY format (a binary encoding of array data and metadata) and the NPZ format (zipped bundles of NPY files). Those same formats, extended in a custom NPZ packaged with JSON metadata, can be used in Python to create a stable DataFrame storage format that can materially out-perform Parquet read / write times in a wide range of contexts. Unlike Parquet, all characteristics of a DataFrame can be encoded and all NumPy dtypes are supported. Implemented in StaticFrame, this format can take advantage of an immutable data model to memory-map full DataFrames from un-zipped directories of NPY. Given wide-spread use of Parquet files in data science workflows and AWS services such as Glue, a faster-than-parquet file format can significantly reduce compute costs. This talk will review the specification, implementation, and performance characteristics of this format.



Description:
    You can use Markdown here. Please do not include any personally identifiable information. The initial round of reviews are anonymous, and this field will be visible to reviewers. This section is also used on the schedule for attendees to view. Be clear and precise when describing your presentation. Please write at most 300 words.


I will begin this talk by introducing the challenge of serializing DataFrames, illustrating how nearly all stable encoding formats lack full support for all DataFrame characteristics. While the broadly-used Parquet format has been called a "gold standard" binary file format, its columnar representation will be shown to have limitations when used for encoding DataFrames.

I will show how the NPY format, combined with JSON metadata, can be used to create a custom NPZ file with significant performance and compatibility advantages compared to Parquet. The details of this encoding scheme will be explained.

I will close the talk by evaluating numerous read / write performance comparisons between Parquet (via Pandas) and NPZ (via StaticFrame), measured with a wide variety of DataFrame shapes and dtype compositions. I will share techniques used in implementing optimized Python routines for reading and writing NPY files, and demonstrate applications for memory-mapping complete DataFrames via the same NPY representation.



Audience
    1–2 paragraphs that should answer three questions: (1) Whom is this presentation for? (2) What background, knowledge, or experience do you expect attendees to have? (3) What do you expect attendees to learn, or to be able to do after attending your presentation?


This presentation is for anyone working in Python with DataFrames (including but not limited to Pandas DataFrames). Those working in NumPy, or using the Parquet format independent of Pandas, will also gain insight into approaches to serializing array data to disk. Those using AWS services such as Glue might also benefit from learning about options beyond Parquet for DataFrame serialization. This talk will assume basic experience with Pandas or related DataFrame libraries. Additional experience with Parquet files is beneficial but not required.

After attending my presentation, attendees will have a complete understanding of DataFrame characteristics and learn how various encoding formats, including Parquet, do not support all of those characteristics. Attendees will learn the history of the NPY format, its low-level specification, and optimizations found in a new implementation of encoders and decoders. Finally, attendees will learn how a custom NPZ format can encode an entire DataFrame, when, why, and by how much this format can outperform Parquet, and how the NPY format offers opportunities for memory mapping a complete DataFrame.





Outline



The challenge of serializing DataFrames
    DataFrames are more than just a table
        Index and columns have types, hierarchies, and name attributes
        Tables themseves might have name attributes or other metadata
    Common formats leave out information
        XLSX: no place for name of index and name of columns
        Database tables: no place for non-string column headers
    Pickling a DataFrame is always fastest
        Pickles not suitable for long-term storage
        Not secure for sharing
    DataFrame enciding and decoding performance is critical to common workflows

NPY, NPZ, & Parquet
    NPY Proposed as part of NEP 1 in 2007
    NPY simple binary format
        array metadata encoded in binary
        array data as contiguous bytes
    NPZ bundles multiple NPZ in a zipped
    NumPy provides bulit-in support: np.save, np.savez, np.load
    NPY supports Structured Arrays for differene columnar types
    NPY support object dtypes through pickling
        Pickles introduce compatibility and security issues
        Can pickle an array directly, to no advantage of NPY
    NPY & NPZ in era of DataFrames
        Despite performance, little widespread use
        CSV, XLSX more commonly used
        No support in Pandas
        Parquet becomes the "gold standard" format
            AWS Glue, Athena, Redshift
    The Rise of Parquet
        Parquet developed out of the Arrow project
        Parquet offers columnar encoding scheme
        Designed for more than tables, including nested structures
        Designed for multi-language support
        Provides one of the fastest formats for serializing DataFrames
        Limitations
            Columns labels can only be strings
            Indices and columns cannot be hierarchical
            No support for additional table metadata
            Not a one-to-one mapping to NumPy dtypes

Using NPY & NPZ to completely encode a DataFrame
    Extending the NPZ format
        Store all Frame and Index components as array NPY files
            Use common naming templates for file names
            Retain block structure
        Store metadata in a JSON file
    DataFrames can store 1D columns and 2D arrays of same-typed columns
        Pandas collects 2D arrays independent of column order
        StaticFrame permits adjacent columns of the same type to be 2D arrays
        By encoding larger 2D arrays, we can gain a performance advantage
    Encding metadata in JSON
        number of blocks
        index and columns depth
        name attributes
        index and columns types
    Encoding values
        Store each block as an NPY
        Store each underlying Index array for index and columns
    Encoding columns and index
        Store underlying arrays
        Do not store the underlying mapping: upfront creation cost

Getting NPY to be faster than Parquet
    NumPy's save / load routines emphasize compatibility, re-written for speed
    Caching NPY metadata

    Sources of Performance
        Homogenized adjacent data enabling 2D arrays
        Single read, no-copy loading
        One-to-one type system




I. The challenge of serializing DataFrames (10 min).
A. The importance of DataFrame encoding and decoding performance.
B. DataFrames are more than just a table.
C. How common encodings formats (CSV, XLSX, HDF5, Parquet) leave out information.
D. Pickling a DataFrame is always fastest, but not secure.

II. NPY & NPZ (5 min).
A. The origin of NPY in NEP 1.
B. Specification of the NPY and NPZ formats.
C. Why NPY & NPZ were not previously used for DataFrames.

III. Using NPY & NPZ to completely encode a DataFrame (10 min).
A. Extending the NPZ format with JSON metadata.
B. Encoding the values, index, and columns.

IV. Performance (5 min).
A. Enhancements to NPY encoding / decoding.
B. Comprehensive DataFrame performance comparisons between NPZ and Parquet.
C. Why NPZ can be faster than Parquet.



Past Experience

    Please summarize your teaching or public speaking experience, as well as your experience with the subject. Provide links to one (or two!) previous presentations by each speaker. If you have any additional notes, they can be placed here as well.

I have presented at numerous national and international conferences in many domains over the last twenty years, and taught as a university professor of music technology for six years, frequently teaching technical topics. I have never presented at a PyCon.

I have been programming in Python since the year 2000, I writing production Python for financial systems for nearly ten years, and I am expert in NumPy, Pandas, StaticFrame, and DataFrame libraries in general.


Examples of recent presentations:

- PyData Global 2021: "Why Datetimes Need Units: Avoiding a Y2262 Problem & Harnessing the Power of NumPy's datetime64": https://zoom.us/rec/share/MhHxZLi-SMkU3Sewhv7MKLWhgS0y0T7E7xFqAWfukUNdUGtFJFcHxJf8g2r_dTqq.cBJaD2SZP5P7eLI9?startTime=1635534301000

- PyData LA 2019: "Fitting Many Dimensions into One: The Promise of Hierarchical Indices for Data Beyond Two Dimensions": https://youtu.be/xX8tXSNDpmE



#------------------------------------------

Employing NumPy's NPY Format for Faster-Than-Parquet DataFrame Serialization


--
NEP 1

    Defines NPY, a simple binary storage format for representing all NumPy arrays

    Defines the NPZ format as a zipped collection of NPY

    An efficient storage format for arrays still supported in NumPy

    As Pandas emerged, usage of NPY eclipsed

        A DataFrame is a lot more than an array

--
Using NPY for complete DataFrame encoding

    All widely used DataFrame encodings (CSV, XLSX, HDF5, Parquet) are incomplete

    Pickles are fastest of all but not safe

    A custom NPZ package can be used for complete DataFrame serialization

        Complete compatibility with NumPy

        Faster than the widely used Parquet in nearly all scenarios

    Not considering Parquet in isolation; Parquet as used for DataFrame serialization

--
About Me

    CTO at Research Affiliates

    Python programmer since 2000

    In 2017 began implementing StaticFrame

        DataFrame library based on an immutable data model

        Supports all NumPy dtypes, even those not supported by Pandas

            Unicode
            datetime64 units other than nanoseconds

        Introduces numerous novel containers for working with collections of DataFrames

    Driven to find a better way to encode DataFrame's from my colleagues requests

--
The problem of serializing DataFrames in Pandas

    No format (except Pickle) supports all DataFrame characteristics

    No format supports all NumPy dtypes

    Serialization of DataFrames is often critical to process performance

--
The Problem with pickle

    Always fastest reading and writing DataFrames

    NumPy C-types are fine for pickling

    NumPy object arays can store arbitrary Python objects

    Python objects from Pickles

        Not safe from untrusted sources

        Not suitable as long-term storage

            Pickles hold references to objects that might go out of date

            Pickles are not compatible across platforms / Python versions






==
How to Serialize a DataFrame

    What are the components of a DataFrame?

    How is column data stored in a DataFrame?


--
The Components of a DataFrame

    Dataframes may have first been presented in 1991 as part of S language

    No standard

    My definition based on StaticFrame, which is a superset of Pandas components

--
The Components of a DataFrame

    A DataFrame is collection columns

    Columns are arrays

    Columns may have heterogenous types

    Row and column axis are labelled with Index objects

    Index objects map "labels" (an array) to row / column positions

    Index objects can have "depth" to support hierarchical indices

    Index objects, and the DataFrame itself, can have `name` attributes

<< diagram of frame >>
<< screen shot of SF display >>

--
How Column Data is Stored in a DataFrame

    Store collections of arrays for column data

    Column data referred to as a "block"

    Consolidating 1D arrays into 2D arrays offers performance

        Reduce interaction with Python interpreter

        Faster applications of NumPy functions

        Faster copying

--
Approaches to Managing Blocks

    Unconsolidated

        Store one array per column

        No aggregation benefit

    Order independent

        Collect same-typed column data into a 2D array per type

        Used by Pandas

        Maximizes opportunity for 2D consolidation

        Forces random-ordered extraction in ordered processing

    Order dependent

        Adjacent columns of the same type can be consolidated

        Used by StaticFrame

        Opportunity for 2D consolidation dependent on adjacency

        Optimizes ordered extraction (important!)


<< diagram of approach to storing blocks >>




<< This could be diagram that we fill in the various parts of the DF skeleton >>

==
Complete DataFrame Serialization

    All values

    All value types

    Index and column labels

    Index and columns types

    Index and column depths

    Index, columns, and frame name is specified

--
Serializing DataFrames: text formats

    CSV, TSV, JSON

    All values encoded as strings

    No type information is encoded

    Index and columns depths are not specified

    Index and columns types are not specified

    Index, columns, and frame name is not completely specified

        Might be able to derive index or columns names but not both

        << diagram to show this? >>

    Always slowest

--
Serializing DataFrames: XLSX

    All values encoded

    Some types encoded

        Support for Booleans, strings, numeric, datetime

        No distinction between int, float, or reduced bit-depths numerics

        No support for full range of dt64 units

    Index and columns depths are not specified

    Index and columns types are not specified

    Index, columns names are not specified

        Frame name implied by sheet name

--
Serializing DataFrames: Database

    All values encoded

    Some types encoded (depending in DB)

    Index depths implied by primary keys, types are specified

    Columns depth always 1, always a string

    Index, columns names are not specified

        Frame name implied by table name


--
Serializing DataFrames: Parquet

    All values encoded

    Some types encoded

        Weak support for all units of datetime64

        No support for fixed-size unicode strings

    Index depth not specified, type implied by columns

    Columns depth always 1, limited to string types

    Index, columns, and frame name is not specified

    Generally the fastest format widely used

--
No Complete Serialization Format for DataFrames

    With Pandas, pickle is the only "complete" serialization format

    While many formats get close, none are complete

    Parquet is attractive for performance, but is not a DataFrame






<< spend some time on Parquet; should this come earliers >>

==
Parquet

    Originally created for use in Apache Hadoop

    Well supported by the Arrow library, PyArrow on Python

    Cross-platform, cross-language support

    A strict columnar representation

        No 2D consolidation

        100 columns of floats is 100 float columns

--
Parquet's usage

    Parquet has been called a "gold standard" binary file format

    Used natively by AWS Glue and related routines

--
Parquet is not a DataFrame

    Arrow's Table is simpler than a DataFrame

    Strict columnar representation does not permit same-type consolidation

    Rows do not have an index

    Row and column indices do not have names





--
The NPY format

    A binary file format

    A header followed by a data payload of bytes

    Header
        A prefix and version number: b'\x93NUMPY' + bytes((1, 0))

        A binary representation of the string representation of a dictionary

            Defines dtype, order, and shape

        Padding to align on 64 byte divisions

    Body

        Raw bytes that make up array data

        Bytes for 2D array collected in Fortran or C ordering

    NPY on disk can be memory mapped

--
Loading an Array from Bytes

    Given bytes, all we need is dtype, order, and shape

    Using the from_bytes method

--
The NPZ format

    A zip bundle of NPY files

    Usage in original specification was not defined

--
NumPy routines for writing NPY and NPZ

    pass






==
--
How to encode a DataFrame as an NPZ

    Store blocks, index labels as NPY

    Define metadata in a JSON

        Name attributes of index, row, Frame

        Index types

    Bundle all NPY and JSON components into a single ZIP


--
A DataFrame as an NPZ

    A DataFrame of 1000 columns might have thousands of NPY

    In StaticFrame, same-typed adjacent columns can be represented as a 2D array

    The greater same-type adjacency, the fewer the number of NPY

--
A DataFrame as a directory of NPY

    Given a directory of NPY on a file system

    Memory mapping of all array data is possible






--
Making NPY and NPZ writing fast

    NumPy's routines are not optimzed for scale

    StaticFrame re-implements NPY, NPZ writing for performance

        Only read/write simplest format (1)

        Do not support structured arrays

        Do not use a regular expression




--
https://arrow.apache.org/docs/python/parquet.html


Pandas encoding in Parquet

Hierarchical indices become columns
Hierarchical columns become tuples of strings


>>> f = ff.parse('s(3,4)|i(IH,(str,int))|c(IH,(str,int))')
>>> f
<Frame>
<IndexHierarchy>         zZbu      zZbu      ztsv      ztsv      <<U4>
                         105269    119909    194224    172133    <int64>
<IndexHierarchy>
zZbu             105269  1930.4    -610.8    694.3     1080.4
zZbu             119909  -1760.34  3243.94   -72.96    2580.34
ztsv             194224  1857.34   -823.14   1826.02   700.42
<<U4>            <int64> <float64> <float64> <float64> <float64>
>>> df = f.to_pandas()
>>> table = pa.Table.from_pandas(df)
>>> table
pyarrow.Table
('zZbu', '105269'): double
('zZbu', '119909'): double
('ztsv', '194224'): double
('ztsv', '172133'): double
__index0__: string
__index1__: int64
----
('zZbu', '105269'): [[1930.4,-1760.34,1857.34]]
('zZbu', '119909'): [[-610.8,3243.94,-823.14]]
('ztsv', '194224'): [[694.3,-72.96,1826.02]]
('ztsv', '172133'): [[1080.4,2580.34,700.42]]
__index0__: [["zZbu","zZbu","ztsv"]]
__index1__: [[105269,119909,194224]]


if attempting to go from DF to parquet, fails of columns in index hierarchy are not all string:

ValueError:
                    parquet must have string column names for all values in
                     each level of the MultiIndex


Because Parquet data needs to be decoded from the Parquet format and compression, it can’t be directly mapped from disk. Thus the memory_map option might perform better on some systems but won’t help much with resident memory consumption.

When using pa.Table.from_pandas to convert to an Arrow table, by default one or more special columns are added to keep track of the index (row labels). Storing the index takes extra space, so if your index is not valuable, you may choose to omit it by passing preserve_index=False


>>> f = ff.parse('s(3,4)|i(IH,(str,int))|c(I,str)')
>>> f.to_pandas().to_parquet('/tmp/foo.parquet')
>>> pfile = pq.ParquetFile('/tmp/foo.parquet')
>>> pfile.metadata
<pyarrow._parquet.FileMetaData object at 0x7fe1229c3a40>
  created_by: parquet-cpp-arrow version 6.0.1
  num_columns: 6
  num_rows: 3
  num_row_groups: 1
  format_version: 1.0
  serialized_size: 3588

Some Parquet readers may only support timestamps stored in millisecond ('ms') or microsecond ('us') resolution. Since pandas uses nanoseconds to represent timestamps, this can occasionally be a nuisance. By default (when writing version 1.0 Parquet files), the nanoseconds will be cast to microseconds (‘us’).

--
https://github.com/apache/parquet-format

Parquet metadata is encoded using Apache Thrift.

Parquet is built to support very efficient compression and encoding schemes. Multiple projects have demonstrated the performance impact of applying the right compression and encoding scheme to the data. Parquet allows compression schemes to be specified on a per-column level,

Hierarchically, a file consists of one or more row groups. A row group contains exactly one column chunk per column. Column chunks contain one or more pages.

There are three types of metadata: file metadata, column (chunk) metadata and page header metadata. All thrift structures are serialized using the TCompactProtocol.




The types supported by the file format are intended to be as minimal as possible, with a focus on how the types effect on disk storage. For example, 16-bit ints are not explicitly supported in the storage format since they are covered by 32-bit ints with an efficient encoding. This reduces the complexity of implementing readers and writers for the format. The types are:

    BOOLEAN: 1 bit boolean
    INT32: 32 bit signed ints
    INT64: 64 bit signed ints
    INT96: 96 bit signed ints
    FLOAT: IEEE 32-bit floating point values
    DOUBLE: IEEE 64-bit floating point values
    BYTE_ARRAY: arbitrarily long byte arrays.


Data pages can be individually checksummed. This allows disabling of checksums at the HDFS file level, to better support single row lookups. Data page checksums are calculated using the standard CRC32 algorithm on the compressed data of a page (not including the page header itself).


















