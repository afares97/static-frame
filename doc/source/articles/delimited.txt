


Building NumPy Arrays from CSV Files Faster than Pandas

Going from CSV to NumPy Arrays, Faster than Pandas

Building a Faster-than-Pandas CSV-to-NumPy Arrays Reader in C

Twenty-Years of CSV Parsing

Combining PyObjects, Structs, and NumPy Arrays to Build a (Mostly) Faster-than-Pandas CSV Reader

A (Mostly) Faster-than-Pandas CSV Reader with Support for All NumPy Array Types

Building a (Mostly) Faster-than-Pandas CSV Reader with Support for All NumPy Dtypes in C

Extending Python's CSV-Reader C-Extension to Make A High-Performance, Delimited-Text-to-Numpy-Array Converter


--

Twenty years ago, in 2003, Python 2.3 was released with built-in support for reading CSV files (``csv.reader()``). The C implementation, proposed in PEP 305, defines a core tokenizer that has been a reference for many subsequent projects. Two commonly needed features were not, however, addressed in that implementation: determining type per column, and converting strings to those types (or complete columns to arrays). Pandas ``pd.read_csv`` implements automatic type conversion and realization of columns as NumPy arrays (delivered in a DataFrame) with generally good performance, though not all NumPy dtypes are supported. While NumPy offers ``loadtxt()`` and ``genfromtxt()`` for similar purposes, the former (recently re-implemented in C) does not implement automatic type conversion, while the later (implemented in Python) does not sufficiently scale.

To support reading delimited files in StaticFrame (a DataFrame library built on an immutable data model), I needed something different. The full configuration options of Python's ``csv.reader()`` needed to be supported; one or more columns might selectively do type discovery; and support for all NumPy dtypes was required.

Following the long tradition of extending ``csv.reader()``, I implemented ``delimited_to_arrays()`` to meet all of these needs. Incorporated in StaticFrame, performance tests across a range of DataFrame shapes and type heterogeneity show significant performance advantages over Pandas. Independent of usage in StaticFrame, ``delimited_to_arrays()`` provides a powerful new resource in going from CSV files to columnar arrays. This presentation will review the background, architecture, and performance characteristics of this new implementation.


The Challange of Building Columnar Arrays from CSV
-----------------------------------------------------

CSV and related delimited text file formats remain a necessary evil. While numerous high-performance binary alternatives exist (such as Parquet or NPZ), CSV is still popular: it is human readable, widely available, and importable by countless applications and libraries.

Importing such text formats in a DataFrame library raise two challenges. First, the data has to be read row by row, while ultimately, data is organized and typed by column. If attempting to load data in single iteration, the number of columns or rows cannot be known in advance. Second, no type information is declared within the CSV; if not explicitly provided to the reader, types must discovered and converted or left as strings.

This implementation extends ``csv.reader()`` to store Unicode code points in a dynamic C-array per column. After the file is read, values in these C-arrays are then converted to C-types and directly inserted into new NumPy array buffers. The usage of dynamically allocated C-arrays, and a dynamically allocated collection of those arrays, permits loading from file and writing to array in two iterations of the data.


Twenty Years of CSV Parsing in Python
----------------------------------------

Based on PEP 305 in 2003, ``csv.reader()`` was implemented as a C-extension and first included in Python 2.3. Both reader and writer expose a variety of configuration options to support the full diversity of delimited file dialects, quoting, and escaping.

The reader returns an iterator of lists of strings, representing the content parsed from each row. While very efficient for returning Python strings, converting these strings into Python types or columnar NumPy arrays is not addressed.

>>> list(csv.reader(StringIO('a,b\nx,y')))
[['a', 'b'], ['x', 'y']]


The C-implementation of the reader implements a character by character state machine tokenizer. The basic approach of this state machine, including some of the same naming, is found in many subsequent implementations, and was reused in the implementation of ``delimited_to_arrays()``.

As CSV files commonly contain columnar-typed data, extending ``csv.reader()`` to build arrays per column is a natural extension. Both Pandas and NumPy have offered resources to meet this need.

Pandas ``pd.read_csv()`` extended ``csv.reader()`` to build array data suitable for returning a DataFrame. An important feature added by Pandas is the ability to automatically determining a type per column.

Pands ``pd.read_csv()`` is narrowly implemented for Pandas DataFrames. As such, not all NumPy dtypes are supported. For example, Pandas returns all string data as NumPy object arrays with Python strings, even if a NumPy Unicode array is explicitly requested.

>>> pd.read_csv(StringIO('a,b\nx,y')).dtypes
a    object
b    object
dtype: object
>>> df = pd.read_csv(StringIO('a,b\nx,y'), dtype=np.dtype('U3'))
TypeError: the dtype <U3 is not supported for parsing

Additionally, the full range of NumPy datetime64 units is not supported; if dates are interpreted, they are always interepreted in NumPy ``datetetime64`` nanosecond units, even if the input string is clearly just a date.

>>> pd.read_csv(StringIO('a,b\n2022-01-02,1984-05-22'), dtype=np.datetime64)
TypeError: the dtype datetime64 is not supported for parsing, pass this column using parse_dates instead
>>> pd.read_csv(StringIO('a,b\n2022-01-02,1984-05-22'), parse_dates=[0, 1]).dtypes
a    datetime64[ns]
b    datetime64[ns]


NumPy offers two functions for converting text to arrays: ``loadtxt()`` and ``genfromtxt()``. While, unlike ``pd.read_csv()``, both support all NumPy dtypes, the former does not perform automatic type discovery, and the later, implemented in Python, does not sufficiently scale.


Implementation of ``delimited_to_arrays()``
-----------------------------------------------

The core CSV file parsing and NumPy array creation is implemented in the C-extension function ``delimited_to_arrays()``, part of the ArrayKit Python package. StaticFrame calls ``delimited_to_arrays()`` to convert column data to immutable NumPy arrays, which are then used to build a complete DataFrame.

In the tradition of Pandas and NumPy, ``delimited_to_arrays`` extends the state machine tokenizer from, and supports the full configuration options of, ``csv.reader()``.

The key to achieving excellent performance in a Python CSV reader C-extension is avoiding, as much as possible, ``PyObject`` creation and memory management. This is achieved in ``delimited_to_arrays()`` by using a family of C struct containers and dynamic C arrays of simple types to store column data.

The core C-struct containers are the dynamically-allocated CodePointLine (CPL) and CodePointGrid (CPG). The CPL is a struct containing two dynamically-allocated C-arrays: ``buffer``, or an array of Unicode code points (Py_UCS4), and ``offsets``, an array of ``Py_ssize_t``. Optionally, if type parsing is active, an additional struct is included in the CPL. Code points read from the delimited file, per column, are written to ``buffer``. At the end of each field, the number of code points in that field is written to ``offsets``. This approach permits rapid sequential access to field data without using null-terminators to partition fields.

A CPG stores one or more CPLs, dynamically allocating new CPLs as new columns are observed. Figure 1 illustrates the contents of these containers.

The processing stages of ``delimited_to_arrays`` are outlined in Figure 2. The input is an iterator of string "records". At initialization, a DelimitedReader is given that iterator, as well as access to a CodePointGrid. The first iteration stage parses the delimited file, calling into the CPG to append code points and offsets to the appropriate CPLs. After record iteration is complete, the second iteration stage converts CPL field data to C-types and writes them in the buffer of a NumPy array. The final output is a list of NumPy arrays.

Figure 3 details the processing of each record in the DelimitedReader. For each code point in the record, a state is determined and the appropriate action taken. A simplified control flow is illustrated: if the code point is not the delimiter, CPG Append Point is called; else, CPG Append Offset is called. The full state machine implementation is far more complicated, handling configurable escaped and quoted values.

Figure 4 details the CPG Append Point process. For the current field number, the CPG is evaluated to determine if a new CPL needs to be dynamically allocated. For the additional code point, the CPL is evaluated to determine if it needs to be resized. Next, if type parsing is activate, the code point is passed to the type parser's Process Char function. The type parsing routine is a simple heuristic approach based on the count of distinguishing characters, determining if a field is a string, float, complex, integer, or Boolean. Finally, the code point is appended to the CPL.

Figure 5 details the CPG Append Offset process. Just as when appending code points, both the CPG and the CPL are evaluated to determine if they need to grow. If type parsing is active, a field type is determined; this field type is evaluated in the context of the line type to resolve a new line type. The offset is appended to the offsets buffer, and the maximum observed offsets is updated if necessary.

The second iteration is bound within the CPG To Array List process. As shown in Figure 6, for each CPL, a dtype is determined. With the dtype and the number of offsets (equivalent to the number of elements in the array), an empty NumPy array of the appropriate dtype and size is created. Using the NumPy C-API's ``PyArray_DATA`` function, raw C array is accessed from the NumPy array. Iterating over offsets in the CPL, code points are converted to C-types with a family of custom functions, and then those C-types are written into the array data array. This loop happens without any creation or memory management of intermediary PyObjects.

After CPL data is converted and written to the NumPy array, the array is appended to Python list. After all CPLs are processed, the Python list is returned to the user.


Performance
----------------

Too often cross-library performance studies use a single table for performance tests. Variation in table shape, as well as variation in columnar type heterogeneity, greatly alter performance, depending on underlying implementation details in DataFrame libraries.

To evaluate the performance of ``delimited_to_arrays()``, and by extension StaticFrame's ``sf.Frame.from_delimited()``, comparison is made to Pandas ``pd.read_csv()``, with the C-engine enabled, across three different shapes of 100 million (1e8) elements: tall (1e5 x 1e3), square (1e4 x 1e4), and wide (1e3 x 1e5). For each of those three shapes, three types of type heterogeneity are evaluated: columnar (mixtures of floats, ints, strings, and Booleans per column), mixed (mixtures of types with some adjacent type similarity), and uniform (all float).

For each DataFrame configuration under test, three approaches to handling type discovery are compared: automatic type detection (type parsing), no type detection (simply loading data as strings), and creation from explicitly provided dtypes.

StaticFrame out-performs Pandas for all square and wide formations, for all approches to type discovery. For tall formations, StaticFrame type parsing is shown to be slightly slower than Pandas, but for all other type discvery approachs, StaticFrame is shown to be faster.

Pandas is shown to be significantly slower in all cases where all data is returned as strings. This is likely due to Pandas returning object arrays of Python string objects, whereas StaticFrame returns NumPy Unicode arrays. The creation of Python objects creates creates significant overhead compared to just writing Unicode code points directly into a Unicode array.


Future Enhancements
-----------------------

In the current implementation of ``delimited_to_arrays``, the second stage of iteration is done sequentially in one thread. As there is a CPL for each column, there might be thousands or hundreds of thousands of columns: converting them to NumPy arrays is a embarrassingly parallel problem. Future work to make this second iteration parallel will further improve performance.

Additionally, the ``buffer`` used in each CPL is four-byte Py_UCS4, regardless if the underlying unicode might fit in smaller Py_UCS2 or Py_UCS1. NumPy's new implementation of ``np.loadtxt`` uses C++ template to explicitly support all three sizes of Unicide code points. The ``delimited_to_arrays`` routine might adopt a similar approach, potentially reducing memory overhead during loading.


Conclusion
-----------------------

While NumPy's arrays provide high-performance processing, NumPy has not provided a robust, high-performance solution to creating arrays from delimited text files. The ArrayKit ``delimited_to_arrays`` process achieves this, with same level of configuribility as found in Python's ``csv.reader``. Employed in StaticFrame, delimited files can be realized as DataFrames significantly faster than Pandas default C-engine.




==
Not used:


One of the PEPs authors, Dave Cole, had since 2000 released a stand-alone C-extension CSV parser. The current Python implementation still bares similarity to the Cole implementation from over 20 years ago.

There have been many attempts to address this challenge. For nearly twenty years, the ``csv.reader`` within the Python standard library has provided high-performance parsing of delimited files, but does not infer or convert types, let alone build NumPy arrays. NumPy's built-in ``genfromtxt()`` and ``loadtxt()`` do not support the full diversity of CSV files commonly encountered. While the Pandas CSV reader offers good performance, it does not support all NumPy array dtypes.


While numerous CSV parses exist, none meet the needs of StaticFrame, a DataFrame library built on an immutable data model. This presentation details the implementation of Python C-extension that provides better-than-pandas performance while supporting all types of NumPy arrays.

With over 50 parameters, ``pd.read_csv()`` is highly configurable, though some parameters, such as ``keep_default_na`` and ``na_values`` and ``na_filter`` are confusingly interrelated, and other parameters, such as ``storage_options`` are only there to support network based input file retrieval.


The ``loadtxt()`` function, while recently reimplemented in C, does not support headers and requires types per column to be specified as structured arrays. It is designed more as a loader of output from the corresponding ``savetxt()`` than general purpose CSV reader. There is not support for automatic type discovery.

>>> np.loadtxt((StringIO('a,b\nx,1.2\ny,nan')), dtype=[('a',str), ('b',float)], delimiter=',', skiprows=1, unpack=True)
[array(['', ''], dtype='<U0'), array([1.2, nan])]

While the ``genfromtxt()`` function offers broad flexibility for a range of delimited files and does support automatic type discovery, it is written in pure Python: its performance scales poorly with large CSV files.

>>> np.genfromtxt((StringIO('a,b\nx,1.2\ny,nan')), dtype=None, delimiter=',', skip_header=1)
array([(b'x', 1.2), (b'y', nan)], dtype=[('f0', 'S1'), ('f1', '<f8')])


Then, in the second iteration, when creating new NumPy arrays, values from those C arrays are directly written to the NumPy array's underlying C-buffer (accessed via ``PyArray_DATA``).

NumPy loadtxt C++ impl: https://github.com/numpy/numpy/blob/main/numpy/core/src/multiarray/textreading/tokenize.cpp

Conversion routines: https://github.com/numpy/numpy/blob/main/numpy/core/src/multiarray/textreading/conversions.c



==

Outline:


Goals for a New Implementation
    Support the full configuration of ``csv.reader`` dialects
    Permit optionally parsing (discovering) types per column
    Support all NumPy dtypes

The Challange of Building Columnar Arrays from CSV
    CSV as a necessary evil
    Cannot know number of columns or rows in advance
    Two full iterations required
        Dyanamic C arrays per column collect Unicode code points
        Values per field converted to C-types and inserted in new NumPy array buffers

Twenty Years of CSV Parsing in Python
    Python 2.3's ``csv.reader`` in 2003
    The ``csv.reader`` state machine tokenizer
    The Pandas ``pd.read_csv()``
        No support for NumPy Unicode arrays
        No support NumPy datetime64 units other than nanonsecond
    NumPy's ``np.loadtxt()``: not automatic type discovery
    NumPy's ``np.genfromtxt()``: insufficient performance at scale

Implementation of ``delimited_to_arrays()``
    C-extension implementation as part of ArrayKit
    Re-use of ``csv.reader()`` tokenizer
    Re-use of all ``csv.reader()`` configuration options
    Core C-struct Containers
        CodePointLine (CPL)
            Dynamic array of Py_UCS4
            Dynamic array of Py_ssize_t
            Optional struct to store type discovery metrics
        CodePointGrid (CPG)
            Public interface to CPLs
            Dynamically create CPLs as needed




Notes:

Presenting on the architecture of C code in a Python C-extensions is difficult. I will support my presentation by using a number of diagrams and flow-charts. Little, if any, C code will be displayed.


Past Experience

    Please summarize your teaching or public speaking experience, as well as your experience with the subject. Provide links to one (or two!) previous presentations by each speaker. If you have any additional notes, they can be placed here as well.


I have presented at numerous national and international conferences in many domains over the last twenty years, and taught as a university professor of music technology for six years, frequently teaching technical topics. I have presented at numerous PyData conferences and at one PyCon.

I have been programming in Python since the year 2000, I have been writing production Python for financial systems for over ten years, and I am expert in NumPy, Pandas, StaticFrame, and DataFrame libraries in general.

Examples of recent presentations:

PyCon US 2022: "Employing NumPy's NPY Format for Faster-Than-Parquet DataFrame Serialization": https://youtu.be/HLH5AwF-jx4

PyData Global 2021: "Why Datetimes Need Units: Avoiding a Y2262 Problem & Harnessing the Power of NumPy's datetime64": https://www.youtube.com/watch?v=jdnr7sgxCQI




























